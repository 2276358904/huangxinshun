{
    "vocab_size": 30522,
    "type_vocab_size": 2,
    "embed_size": 128,
    "hidden_size": 768,
    "num_hidden_layers": 12,
    "num_hidden_groups": 1,
    "num_inner_layers": 1,
    "num_heads": 12,
    "hidden_act": "gelu",
    "hidden_dropout_rate": 0.1,
    "attention_probs_dropout_rate": 0.1,
    "layer_norm_epsilon": 1e-12,
    "max_seq_length": 512,
    "initializer_range": 0.02,
    "use_fft": true
}